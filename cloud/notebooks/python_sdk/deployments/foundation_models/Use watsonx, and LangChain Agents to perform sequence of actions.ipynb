{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# Use watsonx.ai and LangChain Agents to perform sequence of actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook contains the steps and code to demonstrate usage of LangChain Agents with watsonx.ai models.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.10.\n",
    "\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to use language models, e.g. `meta-llama/llama-3-70b-instruct`, to create Structured Chat LangChain Agent with memory, where an Agent may perform sequence of actions based on the model responses and assures that LLM uses chat memory to provide the most accurate answer on the user question.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [LangChain integration](#watsonxllm)\n",
    "- [Prompt template and custom tools](#prompt_template)\n",
    "- [LangChain Agent](#agent)\n",
    "- [Example of Structured Chat](#chat)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install and import the `datasets` and dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install \"ibm-watsonx-ai>=1.0.1\" | tail -n 1\n",
    "!pip install \"pydantic>=1.10.0\" | tail -n 1\n",
    "!pip install \"langchain==0.2.1\" | tail -n 1\n",
    "!pip install \"langchain-ibm==0.1.7\" | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the WML credentials\n",
    "This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the project id\n",
    "The Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"watsonxllm\"></a>\n",
    "## Initialize `WatsonxLLM` instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Set model parameters\n",
    "\n",
    "To set the model parameters we use `GenTextParamsMetaNames` metanames from `ibm-watsonx-ai`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "from langchain_ibm import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performed experiments we found that the following parameters work well with the LangChain Agents created in the further section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenTextParamsMetaNames.MAX_NEW_TOKENS: 200,\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 10,\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: 'greedy',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you need to specify `model_id` that will be used for inferencing. In the following notebook we use `meta-llama/llama-2-70b-chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = WatsonxLLM(\n",
    "    url=credentials.get('url'),\n",
    "    apikey=credentials.get('apikey'),\n",
    "    project_id=project_id,\n",
    "    model_id=\"meta-llama/llama-3-70b-instruct\",\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "\n",
      "A generative AI is a type of artificial intelligence that creates new, original content, such as images, videos, music, text, or code, using machine learning algorithms and statistical models.\n"
     ]
    }
   ],
   "source": [
    "response = llm_chat.invoke('What is a generative AI? Respond concisely.')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prompt_template\"></a>\n",
    "## Prompt template preparation and tools\n",
    "### Create Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A structured chat prompt template is composed of several sections: system prompt, chat history and user input. The following prompt template was prepared based on the LangChain [documentation](https://python.langchain.com/docs/modules/agents/agent_types/structured_chat) and adjusted to work with the chat models available on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant prioritizes caution over usefulness, refusing to answer questions that it considers unsafe, immoral, unethical or dangerous. \n",
    "\n",
    "AI assistant has access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a JSON blob to specify a tool by providing an action key (tool name) and an action input key (tool input). \n",
    "\n",
    "Use only valid \"action\" values: \"Final Answer\" or {tool_names} and provide only one action in every response. Use \"action\" value \"Final Answer\" if you know what to respond. Follow this format:\n",
    "\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Observation: action result\n",
    "\n",
    "Thought: I know what to respond\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Reminder to ALWAYS respond with a valid JSON blob of a single action. Use tools only if necessary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '{input}\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what and use tools only if necessary)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the final structured chat prompt template one can use initialization class method from `ChatPromptTemplate` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", system_prompt\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "        (\"user\", user_input),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], template='Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble-but-knowledgeable. The assistant prioritizes caution over usefulness, refusing to answer questions that it considers unsafe, immoral, unethical or dangerous. \\n\\nAI assistant has access to the following tools:\\n\\n{tools}\\n\\nUse a JSON blob to specify a tool by providing an action key (tool name) and an action input key (tool input). \\n\\nUse only valid \"action\" values: \"Final Answer\" or {tool_names} and provide only one action in every response. Use \"action\" value \"Final Answer\" if you know what to respond. Follow this format:\\n\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\nObservation: action result\\n\\nThought: I know what to respond\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n```\\n\\nReminder to ALWAYS respond with a valid JSON blob of a single action. Use tools only if necessary.\\n')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n{agent_scratchpad}\\n(reminder to respond in a JSON blob no matter what and use tools only if necessary)'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we prepare tools (python functions) that the AI assistant will have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of the given word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"Remove all punctuation symbols from the text.\"\"\"\n",
    "    import re \n",
    "\n",
    "    new_string = re.sub(r'[^\\w\\s]','',text) \n",
    "    return new_string\n",
    "\n",
    "@tool\n",
    "def get_mean_from_sequence(sequence: str) -> float:\n",
    "    \"\"\"Return the mean value from the sequence of numbers.\"\"\"\n",
    "    import numpy\n",
    "    try:\n",
    "        return numpy.mean(numpy.array(sequence.split(','), dtype=float))\n",
    "    except:\n",
    "        return \"Elements of `sequence` cannot be converted to float\"\n",
    "\n",
    "tools_chat = [get_word_length, get_mean_from_sequence, remove_punctuations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we define custom `CustomJSONAgentOutputParser` because sometimes model may return response with additional tokens before action JSON blob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain_core.agents import AgentAction, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomJSONAgentOutputParser(JSONAgentOutputParser):\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        i = text.find(\"{\")\n",
    "\n",
    "        # Sometimes model may return additional tokens before json blob\n",
    "        text = text[i:]\n",
    "        if \"```\" not in text:\n",
    "            text = \"\\n```\\n\" + text\n",
    "        return super().parse(text + \"\\n```\\nObservation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"agent\"></a>\n",
    "## Initialize an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following section is the central part of the presented notebook. Here, we construct LangChain Agent and initialise `AgentExecutor` which manage conversation with AI assistant, that has access to the predefined tools, and human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain.agents.format_scratchpad import format_log_to_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prompt template, we include the tools signature and their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chat = prompt.partial(\n",
    "    tools=render_text_description(list(tools_chat)),\n",
    "    tool_names=\", \".join([t.name for t in tools_chat]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we have all components prepared we can define Chat Agent and initialize `AgentExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting stop sequence for LLM\n",
    "llm_with_stop = llm_chat.bind(stop=[\"}\\n\"])\n",
    "\n",
    "agent_chat = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "    )\n",
    "    | prompt_chat\n",
    "    | llm_with_stop\n",
    "    | CustomJSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor_chat = AgentExecutor(\n",
    "    agent=agent_chat, tools=tools_chat, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use `handle_parsing_errors=True` to ensure that when LLM is not able to determine what step to take because its output does not have the correct structure to be handled  by output parser, the user's input will be sent back to the model with the appropriate information to improve the model reponse. Setitng this parameter to True help to keep chat conversation smooth, but may also result in multiple requests with many tokens being sent to the model and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chat\"></a>\n",
    "## Structured Chat with memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include chat memory in conversation with AI assistant we are going to use `ChatMessageHistory` that simply store chat history in the prompt and send it with every user question to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = ChatMessageHistory()\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor_chat,\n",
    "    get_session_history=lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `session_id` is needed to persist conversation histories. Note that watsonx.ai does not conversation history. Let us start the chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I'm doing well, thank you for asking!\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi! How are you?',\n",
       " 'chat_history': [],\n",
       " 'output': \"I'm doing well, thank you for asking!\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Hi! How are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is a median?',\n",
       " 'chat_history': [HumanMessage(content='Hi! How are you?'),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!\")],\n",
       " 'output': 'The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is a median?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Could you provide some example?',\n",
       " 'chat_history': [HumanMessage(content='Hi! How are you?'),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       "  HumanMessage(content='What is a median?'),\n",
       "  AIMessage(content='The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.')],\n",
       " 'output': \"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Could you provide some example?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use one of the provided tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_mean_from_sequence\",\n",
      "  \"action_input\": \"1, 2, -23, 42\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\u001b[33;1m\u001b[1;3m5.5\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The mean value of the sequence is 5.5.\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the mean value of the sequence `1, 2, -23, 42`.',\n",
       " 'chat_history': [HumanMessage(content='Hi! How are you?'),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       "  HumanMessage(content='What is a median?'),\n",
       "  AIMessage(content='The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.'),\n",
       "  HumanMessage(content='Could you provide some example?'),\n",
       "  AIMessage(content=\"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\")],\n",
       " 'output': 'The mean value of the sequence is 5.5.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Calculate the mean value of the sequence `1, 2, -23, 42`.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"remove_punctuations\",\n",
      "  \"action_input\": \"What, is a. loan?!\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\u001b[38;5;200m\u001b[1;3mWhat is a loan\u001b[0m\u001b[32;1m\u001b[1;3mCould not parse LLM output: \n",
      "```\n",
      "?\n",
      "```\n",
      "Observation:\u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The sentence without punctuation symbols is: What is a loan\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Remove all punctuation symbols from the following sentence `What, is a. loan?!` and return clear sentence.',\n",
       " 'chat_history': [HumanMessage(content='Hi! How are you?'),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       "  HumanMessage(content='What is a median?'),\n",
       "  AIMessage(content='The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.'),\n",
       "  HumanMessage(content='Could you provide some example?'),\n",
       "  AIMessage(content=\"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\"),\n",
       "  HumanMessage(content='Calculate the mean value of the sequence `1, 2, -23, 42`.'),\n",
       "  AIMessage(content='The mean value of the sequence is 5.5.')],\n",
       " 'output': 'The sentence without punctuation symbols is: What is a loan'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Remove all punctuation symbols from the following sentence `What, is a. loan?!` and return clear sentence.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_word_length\",\n",
      "  \"action_input\": \"think\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\u001b[36;1m\u001b[1;3m5\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The length of the word 'think' is 5.\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the length of word `think`? ',\n",
       " 'chat_history': [HumanMessage(content='Hi! How are you?'),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       "  HumanMessage(content='What is a median?'),\n",
       "  AIMessage(content='The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.'),\n",
       "  HumanMessage(content='Could you provide some example?'),\n",
       "  AIMessage(content=\"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\"),\n",
       "  HumanMessage(content='Calculate the mean value of the sequence `1, 2, -23, 42`.'),\n",
       "  AIMessage(content='The mean value of the sequence is 5.5.'),\n",
       "  HumanMessage(content='Remove all punctuation symbols from the following sentence `What, is a. loan?!` and return clear sentence.'),\n",
       "  AIMessage(content='The sentence without punctuation symbols is: What is a loan')],\n",
       " 'output': \"The length of the word 'think' is 5.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is the length of word `think`? \"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_word_length\",\n",
      "  \"action_input\": \"watsonx\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\u001b[36;1m\u001b[1;3m7\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_word_length\",\n",
      "  \"action_input\": \"IBM\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\u001b[36;1m\u001b[1;3m3\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_mean_from_sequence\",\n",
      "  \"action_input\": \"7, 3\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\u001b[33;1m\u001b[1;3m5.0\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The average length of the words 'watsonx' and 'IBM' is 5.0.\"\n",
      "}\n",
      "\n",
      "```\n",
      "Observation:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the average length of words `watsonx` and `IBM`? ',\n",
       " 'chat_history': [HumanMessage(content='Hi! How are you?'),\n",
       "  AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       "  HumanMessage(content='What is a median?'),\n",
       "  AIMessage(content='The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.'),\n",
       "  HumanMessage(content='Could you provide some example?'),\n",
       "  AIMessage(content=\"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\"),\n",
       "  HumanMessage(content='Calculate the mean value of the sequence `1, 2, -23, 42`.'),\n",
       "  AIMessage(content='The mean value of the sequence is 5.5.'),\n",
       "  HumanMessage(content='Remove all punctuation symbols from the following sentence `What, is a. loan?!` and return clear sentence.'),\n",
       "  AIMessage(content='The sentence without punctuation symbols is: What is a loan'),\n",
       "  HumanMessage(content='What is the length of word `think`? '),\n",
       "  AIMessage(content=\"The length of the word 'think' is 5.\")],\n",
       " 'output': \"The average length of the words 'watsonx' and 'IBM' is 5.0.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is the average length of words `watsonx` and `IBM`? \"},\n",
    "    config={\"configurable\": {\"session_id\": \"watsonx\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History of the chat is stored locally in `message_history` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi! How are you?'),\n",
       " AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
       " HumanMessage(content='What is a median?'),\n",
       " AIMessage(content='The median is the middle value in a sequence of numbers when they are arranged in order. If the sequence has an even number of values, the median is the average of the two middle values.'),\n",
       " HumanMessage(content='Could you provide some example?'),\n",
       " AIMessage(content=\"Let's say we have a sequence of numbers: 1, 3, 5, 7, 9. To find the median, we need to arrange them in order, which they already are. Since there are an odd number of values, the middle value is the median. In this case, the median is 5.\"),\n",
       " HumanMessage(content='Calculate the mean value of the sequence `1, 2, -23, 42`.'),\n",
       " AIMessage(content='The mean value of the sequence is 5.5.'),\n",
       " HumanMessage(content='Remove all punctuation symbols from the following sentence `What, is a. loan?!` and return clear sentence.'),\n",
       " AIMessage(content='The sentence without punctuation symbols is: What is a loan'),\n",
       " HumanMessage(content='What is the length of word `think`? '),\n",
       " AIMessage(content=\"The length of the word 'think' is 5.\"),\n",
       " HumanMessage(content='What is the average length of words `watsonx` and `IBM`? '),\n",
       " AIMessage(content=\"The average length of the words 'watsonx' and 'IBM' is 5.0.\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary and next steps\n",
    "\n",
    " You successfully completed this notebook!.\n",
    " \n",
    " You learned how to use LangChain Agents using custom llm WastonxLLM.\n",
    " \n",
    "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Authors:\n",
    " **Mateusz Świtała**, Software Engineer at Watson Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright © 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebook-samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
